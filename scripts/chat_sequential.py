from pathlib import Path

import gradio as gr
import numpy as np
import torch
import torchaudio.transforms as T
import whisper
from huggingface_hub import hf_hub_download
from llama_cpp import Llama
from loguru import logger
from transformers import AutoTokenizer

from kxagent.audio import TTSModel

MEMORY_LENGTH = 2
CONTEXT_SIZE = 2048
MAX_TOKENS = 512

asr = whisper.load_model("small")
tokenizer = AutoTokenizer.from_pretrained("Spiral-AI/anonymous-7b")
llm = Llama.from_pretrained(
    repo_id="Spiral-AI/Anonymous-7b-gguf",
    filename="*.gguf",
    # n_gpu_layers=1,
)

repo_id = "litagin/style_bert_vits2_jvnv"

model_path = hf_hub_download(
    repo_id, filename="jvnv-F1-jp/jvnv-F1-jp_e160_s14000.safetensors"
)
config_path = hf_hub_download(repo_id, filename="jvnv-F1-jp/config.json")
style_vec_path = hf_hub_download(repo_id, filename="jvnv-F1-jp/style_vectors.npy")

tts = TTSModel(
    model_path=Path(model_path),
    config_path=Path(config_path),
    style_vec_path=Path(style_vec_path),
    device="mps",
)
tts.load()


def compose_prompt(history) -> str:
    messages = []
    for user_msg, assistant_msg in history:
        if user_msg:
            messages.append({"role": "user", "content": user_msg})
        if assistant_msg:
            messages.append({"role": "assistant", "content": assistant_msg})

    return tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)

def text2speech(history):
    text = history[-1][1]
    sr, audio = tts.infer(text)
    return sr, audio


def speech2text(audio, history):
    sr, y = audio

    # æ•´æ•°å‹ã‹ã‚‰æµ®å‹•å°æ•°ç‚¹å‹ã¸å¤‰æ›
    y = y.astype(np.float32)
    y /= np.max(np.abs(y))

    # ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆã‚’WhisperãŒå¯¾å¿œã™ã‚‹16kHzã¸ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    y_tensor = torch.from_numpy(y).clone()
    resample_rate = whisper.audio.SAMPLE_RATE
    resampler = T.Resample(sr, resample_rate, dtype=y_tensor.dtype)
    y2_tensor = resampler(y_tensor)
    y2_float = y2_tensor.to("cpu").detach().numpy().copy()

    # éŸ³å£°èªè­˜
    result = asr.transcribe(y2_float, verbose=True, fp16=False, language="ja")
    text = result["text"]
    history += [[text, None]]

    return history


def user(user_message, history):
    return "", history + [[user_message, None]]


def bot(history):
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
    prompt = compose_prompt(history)

    # æ¨è«–
    streamer = llm.create_completion(prompt, max_tokens=MAX_TOKENS, stream=True)

    # æ¨è«–çµæœã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒ è¡¨ç¤º
    history[-1][1] = ""
    for msg in streamer:
        message = msg["choices"][0]
        if "text" in message:
            new_token = message["text"]
            if new_token != "<":
                history[-1][1] += new_token
                yield history


def clear_audio_in():
    return None

def clear_chatbot():
    return []


with gr.Blocks() as demo:
    chatbot = gr.Chatbot(label="ãƒãƒ£ãƒƒãƒˆ")
    msg = gr.Textbox("", label="ã‚ãªãŸã‹ã‚‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")
    clear = gr.Button("ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®æ¶ˆå»")
    audio_in = gr.Audio(sources=["microphone"], label="ğŸ™ï¸")
    audio_out = gr.Audio(type="numpy", label="AIã‹ã‚‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸", autoplay=True)

    # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›æ™‚ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(
        bot, chatbot, chatbot
    ).then(text2speech, chatbot, audio_out)

    # éŸ³å£°å…¥åŠ›æ™‚ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
    audio_in.stop_recording(
        speech2text, [audio_in, chatbot], chatbot, queue=False
    ).then(bot, chatbot, chatbot).then(text2speech, chatbot, audio_out).then(
        clear_audio_in, outputs=[audio_in], queue=False
    )

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®æ¶ˆå»
    clear.click(clear_chatbot, outputs=[chatbot], queue=False)

demo.queue().launch()
